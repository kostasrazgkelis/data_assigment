{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7847109b",
   "metadata": {},
   "source": [
    "# Channel VAS Data assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9035ec7d",
   "metadata": {},
   "source": [
    "Author: Konstantinos Razgkelis \n",
    "\n",
    "Date:   Friday, October 22, 2021\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a6c9d6",
   "metadata": {},
   "source": [
    "Step 1: We need to import the PySpark SQL modules that will be used for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "35a16c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a91aa8",
   "metadata": {},
   "source": [
    "Step 2: We need to start a Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d6f5426",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7accd6",
   "metadata": {},
   "source": [
    "Step 3: We are assigning the schema of the file that will be brought from HDFS. We also clean the unnecessary data of each row by assigning the data type of each column and filtering out the null data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "16fdf876",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_schema = StructType([\n",
    "    StructField('timestamp', TimestampType(), True),\n",
    "    StructField('amount', DecimalType(scale=4), True),\n",
    "    StructField('channel', StringType(), True)\n",
    "])\n",
    "\n",
    "initial_data = spark.read.csv(path=\"/opt/workspace/cvas_data.csv\", sep=\",\", header=False, schema=csv_schema)\n",
    "\n",
    "clean_data = initial_data.filter(     initial_data[\"timestamp\"].isNotNull()\\\n",
    "                                   &    initial_data[\"amount\"].isNotNull()\\\n",
    "                                   &    initial_data[\"channel\"].isNotNull()\\\n",
    "                                   &    initial_data[\"channel\"].rlike(\"(\\w+)\")\n",
    "                                  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260dde00",
   "metadata": {},
   "source": [
    "Bonus step #1: Check the size of the initial and cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "928484b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "064e7a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba99ab11",
   "metadata": {},
   "source": [
    "Step 4: We will be saving the new spark DataFrame to HDFS. We will be also using pandas to save the parquet file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "002591f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = clean_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5e065917",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df.to_parquet(\"/opt/workspace/output.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b4b59a",
   "metadata": {},
   "source": [
    "Step 5: We have our output file in the HDFS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8873d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet = spark.read.parquet(\"file:///opt/workspace/output.parquet\")\n",
    "parquet.createOrReplaceTempView(\"parquet\")\n",
    "parquet.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e575f9c8",
   "metadata": {},
   "source": [
    "Step 6: We can start making SQL queries in order to take results from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "05e22085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-------+\n",
      "|          timestamp|amount|channel|\n",
      "+-------------------+------+-------+\n",
      "|2021-08-15 23:09:14|0.7500|    SMS|\n",
      "|2021-08-15 23:06:27|1.5000|    SMS|\n",
      "|2021-08-16 08:46:51|0.7500|    SMS|\n",
      "|2021-08-16 02:59:14|0.7500|    SMS|\n",
      "|2021-08-16 07:16:02|0.7500|    SMS|\n",
      "|2021-08-16 08:10:23|0.7500|    SMS|\n",
      "|2021-08-16 02:36:15|0.7500|    SMS|\n",
      "|2021-08-16 01:45:30|0.7500|    SMS|\n",
      "|2021-08-16 05:03:57|0.7500|    SMS|\n",
      "|2021-08-16 06:37:47|0.7500|    SMS|\n",
      "|2021-08-16 00:42:31|0.7500|    SMS|\n",
      "|2021-08-16 02:32:25|1.5000|    SMS|\n",
      "|2021-08-16 06:47:11|0.7500|    SMS|\n",
      "|2021-08-16 05:08:18|0.7500|    SMS|\n",
      "|2021-08-16 06:19:31|0.7500|    SMS|\n",
      "|2021-08-15 23:43:09|0.7500|    SMS|\n",
      "|2021-08-16 06:20:50|1.5000|    SMS|\n",
      "|2021-08-16 08:00:19|1.5000|    SMS|\n",
      "|2021-08-16 03:04:01|1.5000|    SMS|\n",
      "|2021-08-16 06:27:55|0.7500|    SMS|\n",
      "+-------------------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM parquet WHERE parquet.amount > '0.3' and parquet.channel = 'SMS' \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec4ded",
   "metadata": {},
   "source": [
    "Step 7: We have finished our job and we just need to close the spark.session so that it can clean the unused files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d75f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f833e98f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
